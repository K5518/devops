# What is GPT-j?

GPT-j is an open-source language model developed by EleutherAI, a community-led initiative focused on creating high-quality, accessible AI models. It is based on the GPT (Generative Pre-trained Transformer) architecture, which was first introduced by OpenAI in 2018.

GPT-j is one of the largest language models available, with 6 billion parameters, making it comparable in size to the largest models developed by OpenAI and Google. The model was trained on a large corpus of text data from a diverse range of sources, including books, websites, and scientific papers.

</br>

# sources for GPT-j:

- [GPT-j-ForeFront](https://www.forefront.ai/blog-posts/how-you-can-use-gpt-j)

- [GPT-j-Forefront](https://www.forefront.ai/blog-posts/gpt-j-6b-an-introduction-to-the-largest-open-sourced-gpt-model)

- [Custom GPT-j](https://www.youtube.com/watch?v=ym6mWwt85iQ)

- [GOT-j by Briliants](https://www.youtube.com/watch?v=07ppAKvOhqk)
