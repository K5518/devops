# VAE (Variational Autoencoder)

VAE is a type of generative model used in machine learning that learns to generate new data similar to the training data.VAEs use neural networks to encode data into a latent space and then decode it back to the original data space.
The encoding step involves learning a probability distribution in the latent space that captures the important features of the data.VAEs use a variational lower bound to optimize the model parameters during training, which involves minimizing the difference between the actual distribution of the latent variables and a target distribution.VAEs can be used for tasks such as image and speech generation, anomaly detection, and data compression.
VAEs have some limitations, such as difficulty in capturing complex dependencies and the tendency to produce blurry or low-quality images. There are many resources available online to learn more about VAEs, including academic papers, tutorials, and code examples.

- [Intuitively Understanding Variational Autoencoders](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)
- [Convolutional Variational Autoencoder](https://www.tensorflow.org/tutorials/generative/cvae)
- [Variational Autoencoders ](https://www.youtube.com/watch?v=FzYBn1slG8w)
- [A variational autoencoder (VAE)](https://www.jeremyjordan.me/variational-autoencoders/)