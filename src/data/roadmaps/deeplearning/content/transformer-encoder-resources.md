# Transformer Encoder 

The Transformer encoder is a type of neural network architecture that was introduced in the paper "Attention is All You Need" by Vaswani et al. in 2017. The Transformer encoder uses self-attention to process input sequences, such as text or speech. Self-attention allows the model to weigh the importance of different parts of the input sequence when making predictions. The Transformer encoder consists of multiple layers, each of which applies a series of operations to the input sequence. These operations typically include self-attention, layer normalization, and feedforward neural networks.

- [The original paper by Vaswani et al. (2017): "Attention is All You Need"](https://arxiv.org/abs/1706.03762)
- [Transformers in NLP: A beginner friendly explanation](https://towardsdatascience.com/transformers-89034557de14)
- [RImplementing the Transformer Encoder](https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras/)
- [Coursera "Attention Mechanisms in Natural Language Processing" course](https://www.coursera.org/lecture/attention-models-in-nlp/transformer-encoder-I7vJT)
- [Transformer Encoder in 100 lines of code!](https://www.youtube.com/watch?v=g3sEsBGkLU0)