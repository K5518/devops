# TRANSFORMER ATTENTION RESOURCES
We will first focus on the Transformer attention mechanism in this tutorial and subsequently review the Transformer model in a separate one.
Before the introduction of the Transformer model, the use of attention for neural machine translation was implemented by RNN-based encoder-decoder architectures. The Transformer model revolutionized the implementation of attention by dispensing with recurrence and convolutions and, alternatively, relying solely on a self-attention mechanism.

 "The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution".

 - [transformers attention resource](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)
 - [transformers attention resource in ml](https://www.dominodatalab.com/blog/transformers-self-attention-to-the-rescue)
 - [The Transformer Attention Mechanism
](https://machinelearningmastery.com/the-transformer-attention-mechanism/)
- [cornell university](https://arxiv.org/abs/2209.09735)
- [Attention Is All You Need - Paper Explained
](https://www.youtube.com/watch?v=XowwKOAWYoQ)
- [youtube resources](https://www.youtube.com/watch?v=XowwKOAWYoQ)