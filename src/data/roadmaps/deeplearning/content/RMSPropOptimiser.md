# RMSprop optimizer

RMSprop is a gradient based optimization technique used in training neural networks. It was proposed by the father of back-propagation, Geoffrey Hinton. Gradients of very complex functions like neural networks have a tendency to either vanish or explode as the data propagates through the function.

- [RMSprop Deepcheck](https://deepchecks.com/glossary/rmsprop/)
- [RMSprop DL](https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/)
- [RMSprop](https://optimization.cbe.cornell.edu/index.php?title=RMSProp)
- [RMSprop Optimizer](https://medium.com/analytics-vidhya/a-complete-guide-to-adam-and-rmsprop-optimizer-75f4502d83be)
- [Yacine Mahdid YB](https://www.youtube.com/watch?v=nLCuzsQaAKE)





