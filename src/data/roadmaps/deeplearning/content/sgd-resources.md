# Stochastic Gradient Descent(SGD)

SGD (Stochastic Gradient Descent) is an optimization algorithm used in machine learning to minimize the loss function of a model by updating its parameters iteratively. It works by randomly selecting a small subset of the training data (a mini-batch) to compute the gradient of the loss function with respect to the model parameters, and then updating the parameters in the direction of the negative gradient.

- ["Deep Learning" on Coursera](https://in.coursera.org/specializations/deep-learning)
- [Convolutional Neural Networks](https://in.coursera.org/learn/convolutional-neural-networks)
- [Stochastic Gradient Descent, Clearly Explained!!!](https://youtu.be/vMh0zPT0tLI)
- [Stochastic gradient descent explained | Stochastic gradient descent vs Gradient descent|Mini batch](https://youtu.be/7QzNifIDXDw)
- [ML | Stochastic Gradient Descent (SGD)](https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/)
- [Stochastic Gradient Descent â€” Clearly Explained !!](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31)
- ["Deep Learning" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville](https://www.amazon.in/Deep-Learning-Ian-Goodfellow/dp/0262035618)
